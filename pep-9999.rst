#PEP: 9999
Title: Improving the bytecode compiler
Author: Mark Shannon <mark@hoypy.org>
        Pablo Galindo <pablogsal@gmail.com>
Status: Draft
Type: Standards Track
Python-Version: 3.9
Content-Type: text/x-rst
Created: 09-Sep-2019
Post-History: ...

Abstract
========

This PEP proposes improving the source code to bytecode compiler, so that it produces better bytecode, is easier to maintain, and is faster.
This will produce a double benefit for performance as code will run faster, thanks to better bytecode, and by taking less time to produce the bytecode.

The current compiler works in three passes. This PEP proposes replacing that with a four pass compiler,
where each pass would be simpler and more focused.

Motivation
==========

The current bytecode compiler pipeline is currently distributed across two files:

* ``Python/compile.c`` contains all the machinery to convert the abstract syntax tree (AST) into a linear
  sequence of bytecode.
* ``Python/peephole.c`` contains the peephole optimizer, which receives the linear sequence of byte code and
  performs many optimizations in it.

Recently, many functionalities of the peephole optimizer have been ported elsewhere. The rationale
behind this decision is that as the peephole optimizer acts on a linear sequence of bytecode,
it has no information of the structure of the code or what original constructs correspond to
the bytecode sequences is operating on. For this reason, many of these operations are much
easier to do (or only possible to) in earlier stages of the compilation process. Some examples
of this tendency are bpo-29469: Move constant folding from the peephole optimizer to the AST
level or bpo-37289: Remove 'if False' handling in the peephole optimizer. Additionally, the
peephole optimizer needs to fix the lnotab code attribute after performing the optimizations.
Doing this operation on the linear sequence of bytecode after the elimination of some of the
elements is very error prone due to the fact that there is no structure information available
for the reconstruction.  One example of the chalenges this raises is bpo-38115: Deal with
invalid bytecode offsets in lnotab, that deals with the peephole optimizer producing bytecode
offsets that are no longer valid.

Historically, compilers usually have explicit structures along the compilation pipeline in
which different optimizations can operate more naturally of. One of these structures is a
control flow graph (CFG). The CFG is a directed graph where the vertices represent basic blocks
and edges represent possible transfer of control flow from one basic block to another. An
important task of each compiler pass is to keep both the control flow graph up-to-date.
Reconstruction of the control flow graph after each pass is not an option, since it may be very
expensive and lost information sometimes cannot be reconstructed at all. The control flow graph
in CPython is not an explicit structure and therefore optimizations that would be more natural
to be applied to the CFG are applied to suboptimal structures resulting in code that is tightly
coupled with the idiosyncracies of the implementation.

...

Rationale
=========

The current bytecode compiler works in three stages:
1. Conversion of the abstract syntax tree (AST) to a control flow graph (CFG)
2. Conversion of the CFG to bytecode and line number table.
3. A "peephole" optimiser that modifies the generated bytecode, performing both peephole optimizations and some branch elimination.

There are two problems with this approach.

1. Creating the CFG directly is inefficient and a bit awkward from point of view of understanding the generated code.
Using labels instead of basic blocks would be both faster and clearer.

2. In its current form the peephole optimizer is both limited and fragile.
It is limited, as it has to work on the generated bytecode, rather than
the control flow graph. It is fragile as it must recreate elements of
the CFG from the bytecode.

By emitting a linear sequence of instructions, with labels for jumps, the AST traversal code can be made cleaner and faster.
Creation of the CFG can then be done by a fast linear pass over the instruction list.

By transforming the CFG instead of the bytecode, the optimizer can both 
perform more optimizations and be simpler. More optimizations means that
CPython will be (a little bit) faster. Simpler means easier to maintain.

Compiler passes
===============

AST traversal
'''''''''''''

Traverse the AST producing a linear sequence of instructions. Will use labels instead of basic blocks as jump targets.

Control flow graph creation
'''''''''''''''''''''''''''

Produce basic blocks from instructions. This is fast as it is performed by linear sweeps over the instruction list, ensuring good locality of memory access.

Optimization
''''''''''''

Perform optimizations on the CFG.

Will require multiple passes, but as there are only two blocks of memory in use during these passes, passes will be fast.

Emit
''''

Order the CFG and compute branch offsets.
Convert CFG to executable bytecode and line number table.

Using a linear sequence of instructions and labels for code generation
======================================================================

Currently, basic blocks are allocated on the fly, meaning that there are many smallish allocations.
By computing the CFG after instruction generation, memory use can be halved.
By allocating both instructions and basic blocks in much larger arrays, it should be significantly faster as well.

Data structures and sizes (for a 64 bit machine)
''''''''''''''''''''''''''''''''''''''''''''''''

================  ============== ===============
 Data structure    Current size   Proposed size
================  ============== ===============
 Instruction        24 bytes       16 bytes or 12 bytes (with PEP 611)
 Basic block        48 bytes       20 bytes
================  ============== ===============


.. code-block::

    struct instruction {
        int opcode;  // Only 8 or 9 bits required
        int operand_or_label; // 22 bits or fewer with PEP 611
        int line_number;
    }

    struct basic_block {
        int32_t start_instruction;
        int32_t end_instruction;
        int32_t fallthrough_block;
        int32_t target_block;
        int32_t flags;
    }

Labels in code generation
'''''''''''''''''''''''''

Instead of using heap allocated blocks as jump targets, labels will be used instead.
Labels are small stack allocated structures of 64 bits or less.

.. code-block::

    struct label {
        int32_t index;
        enum label_state state; /* Unused, targetted, emitted */
    }

Optimizations to be applied
===========================

Optimizations can be grouped into intra-block (peephole) optimizations
and inter-block (control-flow) optimizations

Intra-block optimizations
'''''''''''''''''''''''''

Packing/unpacking elimination
-----------------------------

Removes the creation of a temporary tuples in multiple assignments and calls.

Instruction sequence improvement
--------------------------------

Replacing sequences of ``LOAD_CONST``, ``LOAD_FAST``, ``STORE_FAST`` and stack manipulation
opcodes with more efficient sequences.

Super-instructions
------------------

If the interpreter supports super-instructions, they would be inserted at this stage.

Inter-block optimzations
''''''''''''''''''''''''

Aggressive dead code elimination
--------------------------------

Do not generate bytecode for any basic blocks that are unreachable.

Basic block extension
---------------------

Appends short basic blocks to basic blocks that unconditional branch to them.
This is a more powerful form of branch elimination. It may enable some additional
intra-block optimizations and perform some limited loop-unrolling for very
small loops.

Eliminate condtional branches or replace them with unconditional branches
-------------------------------------------------------------------------

Replace sequences like ``LOAD_CONST True; POP_JUMP_IF_TRUE`` with ``JUMP``
and eliminate sequences like ``LOAD_CONST True; POP_JUMP_IF_FALSE``.

Super-block formation
---------------------

Combining basic blocks into super-blocks (blocks with a single entry, but multiple exits)
would allow some additional intra-block optimisation opportunities.

Comprehension inlining
----------------------

Move evaluation of comprehensions into the enclosing scope where possible.

Optimizations in the current peephole optimizer
'''''''''''''''''''''''''''''''''''''''''''''''

For reference, the optimizations performed by the current
peephole optimizer are listed below. The new optimzer will
perform a superset of these optimzations.

Packing/unpacking elimination
-----------------------------

Removes the creation of a temporary tuple in multiple assignments.

Simple jump elimination
-----------------------

Removes many, but not all jumps to jumps and conditional branches
where the condition is a constant.


Reference Implementation
========================

  To do...


References
==========

Link to wikipedia or similar explaining CFGs and basic blocks.



Copyright
=========

This document is placed in the public domain or under the
CC0-1.0-Universal license, whichever is more permissive.


..
   Local Variables:
   mode: indented-text
   indent-tabs-mode: nil
   sentence-end-double-space: t
   fill-column: 70
   coding: utf-8
   End:
